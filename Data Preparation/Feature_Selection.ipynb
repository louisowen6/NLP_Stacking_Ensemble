{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Feature_Selection.ipynb","provenance":[],"collapsed_sections":["seNg73VhIw-_","tVV9_UJcJBMB","BUEixBb-Ndnj","P7F-R1_YqppK","EvsEOiPjyjj9","zc1qRk6KsJAk","ImZb4KaitW3g","Kr71MHDgsMoD","b3GatBhkxpUO","MGQDOrHN6aEB","UenNiGg89YVr","SaKPFQft_Xrf","5PLRkF10ynYe","wyCXsW1xmAsM","bS1vQS4lcyel"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tVV9_UJcJBMB"},"source":["# Import Packages"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","#Basic\n","import pandas as pd\n","import gensim\n","import nltk\n","import re\n","import numpy as np\n","import math\n","\n","#For Visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","sns.set()\n","\n","#For Data Preparation\n","import itertools\n","from itertools import combinations\n","\n","#For Class Definition\n","from sklearn.base import BaseEstimator, TransformerMixin"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BUEixBb-Ndnj"},"source":["# Import Data"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["df = pd.read_csv('C:/Users\\Louis Owen/Desktop/NLP_Stacking_Ensemble/df_prepared.csv')\n","df=df.drop(['Unnamed: 0'],1)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"P7F-R1_YqppK"},"source":["# Data Cleaning"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"EvsEOiPjyjj9"},"source":["## Basic"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cashtag</th>\n      <th>conversation_parent</th>\n      <th>conversation_replies</th>\n      <th>created_at</th>\n      <th>liked_by_self</th>\n      <th>official_account</th>\n      <th>sentiment</th>\n      <th>sentiment score</th>\n      <th>source</th>\n      <th>spans</th>\n      <th>...</th>\n      <th>SentiWordNet_max_score</th>\n      <th>SentiWordNet_min_score</th>\n      <th>SentiWordNet_pos_ratio</th>\n      <th>SentiWordNet_neg_ratio</th>\n      <th>Avg_TFIDF_1-grams</th>\n      <th>Avg_TFIDF_2-grams</th>\n      <th>Avg_TFIDF_3-grams</th>\n      <th>Avg_TFIDF_4-grams</th>\n      <th>caps_word</th>\n      <th>hashtags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>$NFLX</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>[3pm,24pm)</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>-0.494</td>\n      <td>stocktwits</td>\n      <td>['out $NFLX -.35']</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.275428</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>$PLUG</td>\n      <td>1.0</td>\n      <td>0.266667</td>\n      <td>[0am,9am)</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.403</td>\n      <td>stocktwits</td>\n      <td>['Very intrigued with the technology and growt...</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.984655</td>\n      <td>7.322638</td>\n      <td>7.589614</td>\n      <td>7.589614</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows Ã— 71 columns</p>\n</div>","text/plain":"  cashtag  conversation_parent  conversation_replies  created_at  \\\n0   $NFLX                  NaN              0.000000  [3pm,24pm)   \n1   $PLUG                  1.0              0.266667   [0am,9am)   \n\n   liked_by_self  official_account sentiment  sentiment score      source  \\\n0            0.0               0.0       NaN           -0.494  stocktwits   \n1            0.0               0.0       NaN            0.403  stocktwits   \n\n                                               spans  ...  \\\n0                                 ['out $NFLX -.35']  ...   \n1  ['Very intrigued with the technology and growt...  ...   \n\n  SentiWordNet_max_score  SentiWordNet_min_score SentiWordNet_pos_ratio  \\\n0                    NaN                     NaN                    0.0   \n1                    0.0                     0.0                    0.0   \n\n  SentiWordNet_neg_ratio  Avg_TFIDF_1-grams  Avg_TFIDF_2-grams  \\\n0                    0.0           4.275428                NaN   \n1                    0.0           4.984655           7.322638   \n\n   Avg_TFIDF_3-grams  Avg_TFIDF_4-grams  caps_word  hashtags  \n0                NaN                NaN          0         0  \n1           7.589614           7.589614          0         0  \n\n[2 rows x 71 columns]"},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["df.head(2)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1454 entries, 0 to 1453\nData columns (total 71 columns):\ncashtag                       1454 non-null object\nconversation_parent           163 non-null float64\nconversation_replies          919 non-null float64\ncreated_at                    1454 non-null object\nliked_by_self                 919 non-null float64\nofficial_account              919 non-null float64\nsentiment                     162 non-null object\nsentiment score               1454 non-null float64\nsource                        1454 non-null object\nspans                         1454 non-null object\ntext                          1454 non-null object\ntotal_likes                   919 non-null float64\nclean_text                    1454 non-null object\nbase_text                     1454 non-null object\nPOS_VB                        1454 non-null int64\nPOS_VBD                       1454 non-null int64\nPOS_VBG                       1454 non-null int64\nPOS_VBN                       1454 non-null int64\nPOS_VBP                       1454 non-null int64\nPOS_VBZ                       1454 non-null int64\nPOS_VM                        1454 non-null int64\n+num                          1454 non-null int64\n-num                          1454 non-null int64\nnum%                          1454 non-null int64\n+num%                         1454 non-null int64\n-num%                         1454 non-null int64\n$num                          1454 non-null int64\nword_num                      1454 non-null int64\nordinal_num                   1454 non-null int64\nnum-num                       1454 non-null int64\nnum-num%                      1454 non-null int64\nnum-num-num                   1454 non-null int64\nnum/num                       1454 non-null int64\nnum/num/num                   1454 non-null int64\nonly_number                   1454 non-null int64\ncall_+num%                    1454 non-null int64\ncall_-num%                    1454 non-null int64\nput_+num%                     1454 non-null int64\nput_-num%                     1454 non-null int64\nbull                          1454 non-null int64\nbear                          1454 non-null int64\nnumber_of_!                   1454 non-null int64\nnumber_of_?                   1454 non-null int64\nnumber_of_$                   1454 non-null int64\ncontinous_!                   1454 non-null int64\ncontinous_?                   1454 non-null int64\nAFINN_sum_score               1454 non-null float64\nAFINN_max_score               1454 non-null float64\nAFINN_min_score               1454 non-null float64\nAFINN_pos_ratio               1454 non-null float64\nAFINN_neg_ratio               1454 non-null float64\nBingLiu_pos_ratio             1454 non-null float64\nBingLiu_neg_ratio             1454 non-null float64\nGeneral_Inquirer_pos_ratio    1454 non-null int64\nGeneral_Inquirer_neg_ratio    1454 non-null int64\nNRC_Hashtag_sum_score         1454 non-null float64\nNRC_Hashtag_max_score         1454 non-null float64\nNRC_Hashtag_min_score         1454 non-null float64\nNRC_Hashtag_pos_ratio         1454 non-null float64\nNRC_Hashtag_neg_ratio         1454 non-null float64\nSentiWordNet_sum_score        1454 non-null float64\nSentiWordNet_max_score        1430 non-null float64\nSentiWordNet_min_score        1430 non-null float64\nSentiWordNet_pos_ratio        1454 non-null float64\nSentiWordNet_neg_ratio        1454 non-null float64\nAvg_TFIDF_1-grams             1448 non-null float64\nAvg_TFIDF_2-grams             1427 non-null float64\nAvg_TFIDF_3-grams             1386 non-null float64\nAvg_TFIDF_4-grams             1326 non-null float64\ncaps_word                     1454 non-null int64\nhashtags                      1454 non-null int64\ndtypes: float64(27), int64(36), object(8)\nmemory usage: 806.6+ KB\n"}],"source":["df.info()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zc1qRk6KsJAk"},"source":["## Missing Values Analysis"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":"['conversation_parent',\n 'conversation_replies',\n 'liked_by_self',\n 'official_account',\n 'sentiment',\n 'total_likes',\n 'SentiWordNet_max_score',\n 'SentiWordNet_min_score',\n 'Avg_TFIDF_1-grams',\n 'Avg_TFIDF_2-grams',\n 'Avg_TFIDF_3-grams',\n 'Avg_TFIDF_4-grams']"},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["def missing_values(df):\n","  '''\n","  Function to check features with missing values\n","  '''\n","  missing_values_feat=[]\n","  for column in df.columns:\n","    if df[column].isnull().values.any():\n","      missing_values_feat.append(column)\n","  return(missing_values_feat)\n","\n","missing_values(df)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":"919"},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["len(df[df.source=='stocktwits'])"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>conversation_parent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>163.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.723926</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.448431</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"       conversation_parent\ncount           163.000000\nmean              0.723926\nstd               0.448431\nmin               0.000000\n25%               0.000000\n50%               1.000000\n75%               1.000000\nmax               1.000000"},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["df[['conversation_parent']].describe()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"au9DQyF3N_x_"},"source":["Remove [conversation_parent], because too much missing values"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>conversation_replies</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>919.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.011643</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.052016</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"       conversation_replies\ncount            919.000000\nmean               0.011643\nstd                0.052016\nmin                0.000000\n25%                0.000000\n50%                0.000000\n75%                0.000000\nmax                1.000000"},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["df[['conversation_replies']].describe()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XTdpf5UdOEIT"},"source":["impute -1 for missing values, because the missing values is from twitter data"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>liked_by_self</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>919.0</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"       liked_by_self\ncount          919.0\nmean             0.0\nstd              0.0\nmin              0.0\n25%              0.0\n50%              0.0\n75%              0.0\nmax              0.0"},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["df[['liked_by_self']].describe()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NUL-YTgwObJT"},"source":["Remove [liked_by_self], because the missing value is from twitter data"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>official_account</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>919.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.132753</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.339492</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"       official_account\ncount        919.000000\nmean           0.132753\nstd            0.339492\nmin            0.000000\n25%            0.000000\n50%            0.000000\n75%            0.000000\nmax            1.000000"},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["df[['official_account']].describe()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zMlIKfPsR_vf"},"source":["impute -1 for missing values, because the missing value is from twitter data"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>162</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>Bullish</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>123</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"       sentiment\ncount        162\nunique         2\ntop      Bullish\nfreq         123"},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["df[['sentiment']].describe()"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":"array([nan, 'Bearish', 'Bullish'], dtype=object)"},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["df['sentiment'].unique()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WI9mlRe_OfKw"},"source":["OHE: 1 for bullish -1 for bearish else 0, because the nan values assumed to be neutral"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>total_likes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>919.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.046971</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.124470</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"       total_likes\ncount   919.000000\nmean      0.046971\nstd       0.124470\nmin       0.000000\n25%       0.000000\n50%       0.000000\n75%       0.000000\nmax       1.000000"},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["df[['total_likes']].describe()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Wha1jeaKSCrp"},"source":["impute -1 for missing values, becaue the missing data is from twitter data"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SentiWordNet_max_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1430.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.232955</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.260000</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-0.750000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.125000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.375000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"       SentiWordNet_max_score\ncount             1430.000000\nmean                 0.232955\nstd                  0.260000\nmin                 -0.750000\n25%                  0.000000\n50%                  0.125000\n75%                  0.375000\nmax                  1.000000"},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["df[['SentiWordNet_max_score']].describe()"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cashtag</th>\n      <th>conversation_parent</th>\n      <th>conversation_replies</th>\n      <th>created_at</th>\n      <th>liked_by_self</th>\n      <th>official_account</th>\n      <th>sentiment</th>\n      <th>sentiment score</th>\n      <th>source</th>\n      <th>spans</th>\n      <th>...</th>\n      <th>SentiWordNet_max_score</th>\n      <th>SentiWordNet_min_score</th>\n      <th>SentiWordNet_pos_ratio</th>\n      <th>SentiWordNet_neg_ratio</th>\n      <th>Avg_TFIDF_1-grams</th>\n      <th>Avg_TFIDF_2-grams</th>\n      <th>Avg_TFIDF_3-grams</th>\n      <th>Avg_TFIDF_4-grams</th>\n      <th>caps_word</th>\n      <th>hashtags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>$NFLX</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>[3pm,24pm)</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>-0.494</td>\n      <td>stocktwits</td>\n      <td>['out $NFLX -.35']</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.275428</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>$FCX</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>[9am,3pm)</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.339</td>\n      <td>stocktwits</td>\n      <td>['$FCX +3.53%']</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>172</th>\n      <td>$AAPL</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>[3pm,24pm)</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>-0.270</td>\n      <td>stocktwits</td>\n      <td>['Out $AAPL']</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.275428</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>222</th>\n      <td>$SPY</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>[3pm,24pm)</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.114</td>\n      <td>stocktwits</td>\n      <td>['cover all']</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5.356660</td>\n      <td>7.589614</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>277</th>\n      <td>$NEON</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>[3pm,24pm)</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.430</td>\n      <td>stocktwits</td>\n      <td>['oversold']</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>6.673323</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 71 columns</p>\n</div>","text/plain":"    cashtag  conversation_parent  conversation_replies  created_at  \\\n0     $NFLX                  NaN                   0.0  [3pm,24pm)   \n45     $FCX                  NaN                   0.0   [9am,3pm)   \n172   $AAPL                  NaN                   0.0  [3pm,24pm)   \n222    $SPY                  NaN                   0.0  [3pm,24pm)   \n277   $NEON                  NaN                   0.0  [3pm,24pm)   \n\n     liked_by_self  official_account sentiment  sentiment score      source  \\\n0              0.0               0.0       NaN           -0.494  stocktwits   \n45             0.0               0.0       NaN            0.339  stocktwits   \n172            0.0               0.0       NaN           -0.270  stocktwits   \n222            0.0               0.0       NaN            0.114  stocktwits   \n277            0.0               0.0       NaN            0.430  stocktwits   \n\n                  spans  ... SentiWordNet_max_score  SentiWordNet_min_score  \\\n0    ['out $NFLX -.35']  ...                    NaN                     NaN   \n45      ['$FCX +3.53%']  ...                    NaN                     NaN   \n172       ['Out $AAPL']  ...                    NaN                     NaN   \n222       ['cover all']  ...                    NaN                     NaN   \n277        ['oversold']  ...                    NaN                     NaN   \n\n    SentiWordNet_pos_ratio SentiWordNet_neg_ratio  Avg_TFIDF_1-grams  \\\n0                      0.0                    0.0           4.275428   \n45                     0.0                    0.0                NaN   \n172                    0.0                    0.0           4.275428   \n222                    0.0                    0.0           5.356660   \n277                    0.0                    0.0           6.673323   \n\n     Avg_TFIDF_2-grams  Avg_TFIDF_3-grams  Avg_TFIDF_4-grams  caps_word  \\\n0                  NaN                NaN                NaN          0   \n45                 NaN                NaN                NaN          0   \n172                NaN                NaN                NaN          0   \n222           7.589614                NaN                NaN          0   \n277                NaN                NaN                NaN          0   \n\n     hashtags  \n0           0  \n45          0  \n172         0  \n222         0  \n277         0  \n\n[5 rows x 71 columns]"},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["df[pd.isnull(df.SentiWordNet_max_score)].head()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MXyl237Mmvtn"},"source":["Impute 0 for missing values, because the missing value is because no such word in the sentence that belongs to the sentiwordnet synsets and because 0 is neutral "]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SentiWordNet_min_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1430.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>-0.188199</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.258757</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-1.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-0.375000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.500000</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"       SentiWordNet_min_score\ncount             1430.000000\nmean                -0.188199\nstd                  0.258757\nmin                 -1.000000\n25%                 -0.375000\n50%                  0.000000\n75%                  0.000000\nmax                  0.500000"},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["df[['SentiWordNet_min_score']].describe()"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cashtag</th>\n      <th>conversation_parent</th>\n      <th>conversation_replies</th>\n      <th>created_at</th>\n      <th>liked_by_self</th>\n      <th>official_account</th>\n      <th>sentiment</th>\n      <th>sentiment score</th>\n      <th>source</th>\n      <th>spans</th>\n      <th>...</th>\n      <th>SentiWordNet_max_score</th>\n      <th>SentiWordNet_min_score</th>\n      <th>SentiWordNet_pos_ratio</th>\n      <th>SentiWordNet_neg_ratio</th>\n      <th>Avg_TFIDF_1-grams</th>\n      <th>Avg_TFIDF_2-grams</th>\n      <th>Avg_TFIDF_3-grams</th>\n      <th>Avg_TFIDF_4-grams</th>\n      <th>caps_word</th>\n      <th>hashtags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>$NFLX</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>[3pm,24pm)</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>-0.494</td>\n      <td>stocktwits</td>\n      <td>['out $NFLX -.35']</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.275428</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>$FCX</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>[9am,3pm)</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.339</td>\n      <td>stocktwits</td>\n      <td>['$FCX +3.53%']</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>172</th>\n      <td>$AAPL</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>[3pm,24pm)</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>-0.270</td>\n      <td>stocktwits</td>\n      <td>['Out $AAPL']</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.275428</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>222</th>\n      <td>$SPY</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>[3pm,24pm)</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.114</td>\n      <td>stocktwits</td>\n      <td>['cover all']</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5.356660</td>\n      <td>7.589614</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>277</th>\n      <td>$NEON</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>[3pm,24pm)</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.430</td>\n      <td>stocktwits</td>\n      <td>['oversold']</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>6.673323</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 71 columns</p>\n</div>","text/plain":"    cashtag  conversation_parent  conversation_replies  created_at  \\\n0     $NFLX                  NaN                   0.0  [3pm,24pm)   \n45     $FCX                  NaN                   0.0   [9am,3pm)   \n172   $AAPL                  NaN                   0.0  [3pm,24pm)   \n222    $SPY                  NaN                   0.0  [3pm,24pm)   \n277   $NEON                  NaN                   0.0  [3pm,24pm)   \n\n     liked_by_self  official_account sentiment  sentiment score      source  \\\n0              0.0               0.0       NaN           -0.494  stocktwits   \n45             0.0               0.0       NaN            0.339  stocktwits   \n172            0.0               0.0       NaN           -0.270  stocktwits   \n222            0.0               0.0       NaN            0.114  stocktwits   \n277            0.0               0.0       NaN            0.430  stocktwits   \n\n                  spans  ... SentiWordNet_max_score  SentiWordNet_min_score  \\\n0    ['out $NFLX -.35']  ...                    NaN                     NaN   \n45      ['$FCX +3.53%']  ...                    NaN                     NaN   \n172       ['Out $AAPL']  ...                    NaN                     NaN   \n222       ['cover all']  ...                    NaN                     NaN   \n277        ['oversold']  ...                    NaN                     NaN   \n\n    SentiWordNet_pos_ratio SentiWordNet_neg_ratio  Avg_TFIDF_1-grams  \\\n0                      0.0                    0.0           4.275428   \n45                     0.0                    0.0                NaN   \n172                    0.0                    0.0           4.275428   \n222                    0.0                    0.0           5.356660   \n277                    0.0                    0.0           6.673323   \n\n     Avg_TFIDF_2-grams  Avg_TFIDF_3-grams  Avg_TFIDF_4-grams  caps_word  \\\n0                  NaN                NaN                NaN          0   \n45                 NaN                NaN                NaN          0   \n172                NaN                NaN                NaN          0   \n222           7.589614                NaN                NaN          0   \n277                NaN                NaN                NaN          0   \n\n     hashtags  \n0           0  \n45          0  \n172         0  \n222         0  \n277         0  \n\n[5 rows x 71 columns]"},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["df[pd.isnull(df.SentiWordNet_min_score)].head()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"eaEAjGyumyUQ"},"source":["Impute 0 for missing values, because the missing value is because no such word in the sentence that belongs to the sentiwordnet synsets and because 0 is neutral "]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Avg_TFIDF_1-grams</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1448.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>5.306872</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.795715</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.981975</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>4.810385</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>5.243674</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>5.697248</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>15.179228</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"       Avg_TFIDF_1-grams\ncount        1448.000000\nmean            5.306872\nstd             0.795715\nmin             1.981975\n25%             4.810385\n50%             5.243674\n75%             5.697248\nmax            15.179228"},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["df[['Avg_TFIDF_1-grams']].describe()"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cashtag</th>\n      <th>conversation_parent</th>\n      <th>conversation_replies</th>\n      <th>created_at</th>\n      <th>liked_by_self</th>\n      <th>official_account</th>\n      <th>sentiment</th>\n      <th>sentiment score</th>\n      <th>source</th>\n      <th>spans</th>\n      <th>...</th>\n      <th>SentiWordNet_max_score</th>\n      <th>SentiWordNet_min_score</th>\n      <th>SentiWordNet_pos_ratio</th>\n      <th>SentiWordNet_neg_ratio</th>\n      <th>Avg_TFIDF_1-grams</th>\n      <th>Avg_TFIDF_2-grams</th>\n      <th>Avg_TFIDF_3-grams</th>\n      <th>Avg_TFIDF_4-grams</th>\n      <th>caps_word</th>\n      <th>hashtags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>45</th>\n      <td>$FCX</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>[9am,3pm)</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.339</td>\n      <td>stocktwits</td>\n      <td>['$FCX +3.53%']</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>291</th>\n      <td>$CAT</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>[9am,3pm)</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.586</td>\n      <td>stocktwits</td>\n      <td>['$CAT +5.10%']</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>295</th>\n      <td>$X</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>[9am,3pm)</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.144</td>\n      <td>stocktwits</td>\n      <td>['$X +2.27%']</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>761</th>\n      <td>$YHOO</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>[9am,3pm)</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.160</td>\n      <td>stocktwits</td>\n      <td>['$YHOO +2.61%']</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1230</th>\n      <td>$SBUX</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[9am,3pm)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-0.170</td>\n      <td>twitter</td>\n      <td>['Deutsche Bank: cuts #Starbucks to Hold', 'pr...</td>\n      <td>...</td>\n      <td>0.375</td>\n      <td>0.375</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1302</th>\n      <td>$INTU</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[3pm,24pm)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.161</td>\n      <td>twitter</td>\n      <td>['now a cloud software company. How the compan...</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>6 rows Ã— 71 columns</p>\n</div>","text/plain":"     cashtag  conversation_parent  conversation_replies  created_at  \\\n45      $FCX                  NaN                   0.0   [9am,3pm)   \n291     $CAT                  NaN                   0.0   [9am,3pm)   \n295       $X                  NaN                   0.0   [9am,3pm)   \n761    $YHOO                  NaN                   0.0   [9am,3pm)   \n1230   $SBUX                  NaN                   NaN   [9am,3pm)   \n1302   $INTU                  NaN                   NaN  [3pm,24pm)   \n\n      liked_by_self  official_account sentiment  sentiment score      source  \\\n45              0.0               0.0       NaN            0.339  stocktwits   \n291             0.0               0.0       NaN            0.586  stocktwits   \n295             0.0               0.0       NaN            0.144  stocktwits   \n761             0.0               0.0       NaN            0.160  stocktwits   \n1230            NaN               NaN       NaN           -0.170     twitter   \n1302            NaN               NaN       NaN            0.161     twitter   \n\n                                                  spans  ...  \\\n45                                      ['$FCX +3.53%']  ...   \n291                                     ['$CAT +5.10%']  ...   \n295                                       ['$X +2.27%']  ...   \n761                                    ['$YHOO +2.61%']  ...   \n1230  ['Deutsche Bank: cuts #Starbucks to Hold', 'pr...  ...   \n1302  ['now a cloud software company. How the compan...  ...   \n\n     SentiWordNet_max_score  SentiWordNet_min_score SentiWordNet_pos_ratio  \\\n45                      NaN                     NaN                    0.0   \n291                     NaN                     NaN                    0.0   \n295                     NaN                     NaN                    0.0   \n761                     NaN                     NaN                    0.0   \n1230                  0.375                   0.375                    1.0   \n1302                    NaN                     NaN                    0.0   \n\n     SentiWordNet_neg_ratio  Avg_TFIDF_1-grams  Avg_TFIDF_2-grams  \\\n45                      0.0                NaN                NaN   \n291                     0.0                NaN                NaN   \n295                     0.0                NaN                NaN   \n761                     0.0                NaN                NaN   \n1230                    0.0                NaN                NaN   \n1302                    0.0                NaN                NaN   \n\n      Avg_TFIDF_3-grams  Avg_TFIDF_4-grams  caps_word  hashtags  \n45                  NaN                NaN          0         0  \n291                 NaN                NaN          0         0  \n295                 NaN                NaN          0         0  \n761                 NaN                NaN          0         0  \n1230                NaN                NaN          1         1  \n1302                NaN                NaN          0         0  \n\n[6 rows x 71 columns]"},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["df[pd.isnull(df['Avg_TFIDF_1-grams'])]"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lraWJ6SPmJ5b"},"source":["Impute 0 for all tf-idf missing values, because the missing value is because no such word which pass the n-gram filter and 0 is the neutral value"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ImZb4KaitW3g"},"source":["## Missing Values Imputation"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tTOBXBDktMgY"},"source":["Missing Value Actionable Items:\n","1.   Remove [conversation_parent], because too much missing values\n","2.   Impute -1 for [conversation_replies] missing values, because the missing values is from twitter data\n","3.   Remove [liked_by_self], because constant feature\n","4.   Impute -1 for [official_account] missing values, because the missing value is from twitter data\n","5.   OHE [sentiment]: 1 for bullish -1 for bearish else 0, because the nan values assumed to be neutral\n","6.   Impute -1 for [total_likes] missing values, becaue the missing data is from twitter data\n","7.   Impute 0 for [SentiWordNet_max_score] missing values, because the missing value is because no such word in the sentence that belongs to the sentiwordnet synsets and becaue 0 is neutral \n","8.   Impute 0 for [SentiWordNet_min_score] missing values, because the missing value is because no such word in the sentence that belongs to the sentiwordnet synsets and becaue 0 is neutral \n","9.   Impute 0 for all tf-idf missing values, because the missing value is because no such word which pass the n-gram filter and 0 is the neutral value\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["df=df.drop(['conversation_parent','liked_by_self'],1)\n","df['conversation_replies']=df['conversation_replies'].fillna(-1)\n","df['official_account']=df['official_account'].fillna(-1)\n","df['sentiment']=df['sentiment'].apply(lambda x: 1 if x=='Bullish' else -1 if x=='Bearish' else 0)\n","df['total_likes']=df['total_likes'].fillna(-1)\n","df['SentiWordNet_max_score']=df['SentiWordNet_max_score'].fillna(0)\n","df['SentiWordNet_min_score']=df['SentiWordNet_min_score'].fillna(0)\n","df['Avg_TFIDF_1-grams']=df['Avg_TFIDF_1-grams'].fillna(0)\n","df['Avg_TFIDF_2-grams']=df['Avg_TFIDF_2-grams'].fillna(0)\n","df['Avg_TFIDF_3-grams']=df['Avg_TFIDF_3-grams'].fillna(0)\n","df['Avg_TFIDF_4-grams']=df['Avg_TFIDF_4-grams'].fillna(0)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1454 entries, 0 to 1453\nData columns (total 69 columns):\ncashtag                       1454 non-null object\nconversation_replies          1454 non-null float64\ncreated_at                    1454 non-null object\nofficial_account              1454 non-null float64\nsentiment                     1454 non-null int64\nsentiment score               1454 non-null float64\nsource                        1454 non-null object\nspans                         1454 non-null object\ntext                          1454 non-null object\ntotal_likes                   1454 non-null float64\nclean_text                    1454 non-null object\nbase_text                     1454 non-null object\nPOS_VB                        1454 non-null int64\nPOS_VBD                       1454 non-null int64\nPOS_VBG                       1454 non-null int64\nPOS_VBN                       1454 non-null int64\nPOS_VBP                       1454 non-null int64\nPOS_VBZ                       1454 non-null int64\nPOS_VM                        1454 non-null int64\n+num                          1454 non-null int64\n-num                          1454 non-null int64\nnum%                          1454 non-null int64\n+num%                         1454 non-null int64\n-num%                         1454 non-null int64\n$num                          1454 non-null int64\nword_num                      1454 non-null int64\nordinal_num                   1454 non-null int64\nnum-num                       1454 non-null int64\nnum-num%                      1454 non-null int64\nnum-num-num                   1454 non-null int64\nnum/num                       1454 non-null int64\nnum/num/num                   1454 non-null int64\nonly_number                   1454 non-null int64\ncall_+num%                    1454 non-null int64\ncall_-num%                    1454 non-null int64\nput_+num%                     1454 non-null int64\nput_-num%                     1454 non-null int64\nbull                          1454 non-null int64\nbear                          1454 non-null int64\nnumber_of_!                   1454 non-null int64\nnumber_of_?                   1454 non-null int64\nnumber_of_$                   1454 non-null int64\ncontinous_!                   1454 non-null int64\ncontinous_?                   1454 non-null int64\nAFINN_sum_score               1454 non-null float64\nAFINN_max_score               1454 non-null float64\nAFINN_min_score               1454 non-null float64\nAFINN_pos_ratio               1454 non-null float64\nAFINN_neg_ratio               1454 non-null float64\nBingLiu_pos_ratio             1454 non-null float64\nBingLiu_neg_ratio             1454 non-null float64\nGeneral_Inquirer_pos_ratio    1454 non-null int64\nGeneral_Inquirer_neg_ratio    1454 non-null int64\nNRC_Hashtag_sum_score         1454 non-null float64\nNRC_Hashtag_max_score         1454 non-null float64\nNRC_Hashtag_min_score         1454 non-null float64\nNRC_Hashtag_pos_ratio         1454 non-null float64\nNRC_Hashtag_neg_ratio         1454 non-null float64\nSentiWordNet_sum_score        1454 non-null float64\nSentiWordNet_max_score        1454 non-null float64\nSentiWordNet_min_score        1454 non-null float64\nSentiWordNet_pos_ratio        1454 non-null float64\nSentiWordNet_neg_ratio        1454 non-null float64\nAvg_TFIDF_1-grams             1454 non-null float64\nAvg_TFIDF_2-grams             1454 non-null float64\nAvg_TFIDF_3-grams             1454 non-null float64\nAvg_TFIDF_4-grams             1454 non-null float64\ncaps_word                     1454 non-null int64\nhashtags                      1454 non-null int64\ndtypes: float64(25), int64(37), object(7)\nmemory usage: 783.9+ KB\n"}],"source":["df.info()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Kr71MHDgsMoD"},"source":["## Constant Features"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["class remove_constant_features(BaseEstimator,TransformerMixin):\n","  '''\n","  Class for removing constant features either categorical or numeric features\n","  \n","  Tolerance==0.01 means the numeric features is 99% constant\n","  '''\n","  def __init__(self,tolerance,verbose=False):\n","    self.verbose=verbose\n","    self.tolerance=tolerance\n","  \n","  def fit(self,df):\n","    self.df=df\n","    self.num_constant_features=[]\n","    self.cat_constant_features=[]\n","    columns=self.df.columns.tolist()\n","    for column in columns:\n","      # check constant features for numerical columns\n","      if np.issubdtype(self.df[column].dtype, np.number):\n","        if self.df[column].std()<=self.tolerance:\n","          self.num_constant_features.append(column)\n","      else:\n","        try:\n","          # check constant features for categorical columns\n","          if len(self.df[feat].unique())== 1:\n","            self.cat_constant_features.append(column)\n","        except:\n","          None\n","    return(self)\n","  \n","  def transform(self,df):\n","    self.df=self.df.drop(self.num_constant_features,1)\n","    self.df=self.df.drop(self.cat_constant_features,1)\n","    if self.verbose:\n","      print('Removed Features: ',self.num_constant_features+self.cat_constant_features)\n","      print('')\n","    return(self.df)"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Removed Features:  ['POS_VM', '$num', 'num/num/num', 'call_-num%', 'General_Inquirer_pos_ratio', 'General_Inquirer_neg_ratio']\n\n"}],"source":["constant=remove_constant_features(tolerance=0.01,verbose=True)\n","df=constant.fit_transform(df.copy())"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1454 entries, 0 to 1453\nData columns (total 63 columns):\ncashtag                   1454 non-null object\nconversation_replies      1454 non-null float64\ncreated_at                1454 non-null object\nofficial_account          1454 non-null float64\nsentiment                 1454 non-null int64\nsentiment score           1454 non-null float64\nsource                    1454 non-null object\nspans                     1454 non-null object\ntext                      1454 non-null object\ntotal_likes               1454 non-null float64\nclean_text                1454 non-null object\nbase_text                 1454 non-null object\nPOS_VB                    1454 non-null int64\nPOS_VBD                   1454 non-null int64\nPOS_VBG                   1454 non-null int64\nPOS_VBN                   1454 non-null int64\nPOS_VBP                   1454 non-null int64\nPOS_VBZ                   1454 non-null int64\n+num                      1454 non-null int64\n-num                      1454 non-null int64\nnum%                      1454 non-null int64\n+num%                     1454 non-null int64\n-num%                     1454 non-null int64\nword_num                  1454 non-null int64\nordinal_num               1454 non-null int64\nnum-num                   1454 non-null int64\nnum-num%                  1454 non-null int64\nnum-num-num               1454 non-null int64\nnum/num                   1454 non-null int64\nonly_number               1454 non-null int64\ncall_+num%                1454 non-null int64\nput_+num%                 1454 non-null int64\nput_-num%                 1454 non-null int64\nbull                      1454 non-null int64\nbear                      1454 non-null int64\nnumber_of_!               1454 non-null int64\nnumber_of_?               1454 non-null int64\nnumber_of_$               1454 non-null int64\ncontinous_!               1454 non-null int64\ncontinous_?               1454 non-null int64\nAFINN_sum_score           1454 non-null float64\nAFINN_max_score           1454 non-null float64\nAFINN_min_score           1454 non-null float64\nAFINN_pos_ratio           1454 non-null float64\nAFINN_neg_ratio           1454 non-null float64\nBingLiu_pos_ratio         1454 non-null float64\nBingLiu_neg_ratio         1454 non-null float64\nNRC_Hashtag_sum_score     1454 non-null float64\nNRC_Hashtag_max_score     1454 non-null float64\nNRC_Hashtag_min_score     1454 non-null float64\nNRC_Hashtag_pos_ratio     1454 non-null float64\nNRC_Hashtag_neg_ratio     1454 non-null float64\nSentiWordNet_sum_score    1454 non-null float64\nSentiWordNet_max_score    1454 non-null float64\nSentiWordNet_min_score    1454 non-null float64\nSentiWordNet_pos_ratio    1454 non-null float64\nSentiWordNet_neg_ratio    1454 non-null float64\nAvg_TFIDF_1-grams         1454 non-null float64\nAvg_TFIDF_2-grams         1454 non-null float64\nAvg_TFIDF_3-grams         1454 non-null float64\nAvg_TFIDF_4-grams         1454 non-null float64\ncaps_word                 1454 non-null int64\nhashtags                  1454 non-null int64\ndtypes: float64(25), int64(31), object(7)\nmemory usage: 715.7+ KB\n"}],"source":["df.info()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"b3GatBhkxpUO"},"source":["## Duplicated Features"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["class remove_duplicated_features(BaseEstimator,TransformerMixin):\n","  '''\n","  Class for removing duplicated features\n","  '''\n","  def __init__(self,verbose=False):\n","    self.verbose=verbose\n","    \n","  def fit(self,df):\n","    self.df=df\n","    self.duplicated_feat = []\n","    for i in range(0, len(self.df.columns)):\n","      col_1 = self.df.columns[i]\n","      \n","      for col_2 in self.df.columns[i + 1:]:\n","        # if the features are duplicated\n","        if self.df[col_1].equals(self.df[col_2]):\n","            self.duplicated_feat.append(col_2)\n","    return(self)\n","  \n","  def transform(self,df):\n","    self.df=self.df.drop(self.duplicated_feat,1)\n","    if self.verbose:\n","      print('Removed Features: ',self.duplicated_feat)\n","      print('')\n","    return(self.df)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Removed Features:  []\n\n"}],"source":["duplic=remove_duplicated_features(verbose=True)\n","df=duplic.fit_transform(df.copy())"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MGQDOrHN6aEB"},"source":["## Encoding"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["df['created_at']=df['created_at'].apply(lambda x: 0 if x=='[0am,9am)' else 1 if x=='[9am,3pm)' else 2)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UenNiGg89YVr"},"source":["## Correlated Features\n"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["def correlated(df,tolerance):\n","  '''\n","  Function to know correlated variables\n","  Input: {dataframe, correlation_tolerance}\n","  '''\n","  cate_features_index = np.where(df.dtypes == 'O')[0]\n","  num_features_index = [x for x in range(len(df.columns)) if x not in cate_features_index]\n","  \n","  #Check correlation of numerical variables\n","  corrmat = df.iloc[:,num_features_index].corr()\n","  corrmat = corrmat.abs().unstack() # absolute value of corr coef\n","  corrmat = corrmat.sort_values(ascending=False)\n","  corrmat = corrmat[corrmat >= tolerance]\n","  corrmat = corrmat[corrmat < 1]\n","  corrmat = pd.DataFrame(corrmat).reset_index()\n","  corrmat.columns = ['feature1', 'feature2', 'corr']\n","\n","  # find groups of correlated features\n","\n","  grouped_feature_ls = []\n","  correlated_groups = []\n","\n","  for feature in corrmat.feature1.unique():\n","    if feature not in grouped_feature_ls:\n","      # find all features correlated to a single feature\n","      correlated_block = corrmat[corrmat.feature1 == feature]\n","      grouped_feature_ls = grouped_feature_ls + list(\n","      correlated_block.feature2.unique()) + [feature]\n","\n","      # append the block of features to the list\n","      correlated_groups.append(correlated_block)\n","\n","  print('found {} correlated groups'.format(len(correlated_groups)))\n","  print('out of {} total features'.format(df.iloc[:,num_features_index].shape[1]))\n","\n","  for group in correlated_groups:\n","    print(group)\n","    print()"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"found 1 correlated groups\nout of 57 total features\n               feature1     feature2      corr\n0  conversation_replies  total_likes  0.980199\n\n"}],"source":["correlated(df.drop('source',1),0.9)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VQd4ogYn-pPY"},"source":["Remove conversation_replies, num%"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["df=df.drop(['conversation_replies','num%'],1)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SaKPFQft_Xrf"},"source":["## Final Touch"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["df['spans']=df['spans'].apply(lambda x: x[2:-2])"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5PLRkF10ynYe"},"source":["# Data Report"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cashtag</th>\n      <th>created_at</th>\n      <th>official_account</th>\n      <th>sentiment</th>\n      <th>sentiment score</th>\n      <th>source</th>\n      <th>spans</th>\n      <th>text</th>\n      <th>total_likes</th>\n      <th>clean_text</th>\n      <th>...</th>\n      <th>SentiWordNet_max_score</th>\n      <th>SentiWordNet_min_score</th>\n      <th>SentiWordNet_pos_ratio</th>\n      <th>SentiWordNet_neg_ratio</th>\n      <th>Avg_TFIDF_1-grams</th>\n      <th>Avg_TFIDF_2-grams</th>\n      <th>Avg_TFIDF_3-grams</th>\n      <th>Avg_TFIDF_4-grams</th>\n      <th>caps_word</th>\n      <th>hashtags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>$NFLX</td>\n      <td>2</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>-0.494</td>\n      <td>stocktwits</td>\n      <td>out $NFLX -.35</td>\n      <td>out $NFLX -.35</td>\n      <td>0.0</td>\n      <td>out $NFLX -.35</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.275428</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>$PLUG</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.403</td>\n      <td>stocktwits</td>\n      <td>Very intrigued with the technology and growth ...</td>\n      <td>Been doing some work on $PLUG this evening. Ve...</td>\n      <td>1.0</td>\n      <td>Been doing some work on $PLUG this evening Ver...</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.984655</td>\n      <td>7.322638</td>\n      <td>7.589614</td>\n      <td>7.589614</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows Ã— 61 columns</p>\n</div>","text/plain":"  cashtag  created_at  official_account  sentiment  sentiment score  \\\n0   $NFLX           2               0.0          0           -0.494   \n1   $PLUG           0               0.0          0            0.403   \n\n       source                                              spans  \\\n0  stocktwits                                     out $NFLX -.35   \n1  stocktwits  Very intrigued with the technology and growth ...   \n\n                                                text  total_likes  \\\n0                                     out $NFLX -.35          0.0   \n1  Been doing some work on $PLUG this evening. Ve...          1.0   \n\n                                          clean_text  ...  \\\n0                                     out $NFLX -.35  ...   \n1  Been doing some work on $PLUG this evening Ver...  ...   \n\n  SentiWordNet_max_score  SentiWordNet_min_score  SentiWordNet_pos_ratio  \\\n0                    0.0                     0.0                     0.0   \n1                    0.0                     0.0                     0.0   \n\n   SentiWordNet_neg_ratio  Avg_TFIDF_1-grams  Avg_TFIDF_2-grams  \\\n0                     0.0           4.275428           0.000000   \n1                     0.0           4.984655           7.322638   \n\n   Avg_TFIDF_3-grams  Avg_TFIDF_4-grams  caps_word  hashtags  \n0           0.000000           0.000000          0         0  \n1           7.589614           7.589614          0         0  \n\n[2 rows x 61 columns]"},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["df.head(2)"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1454 entries, 0 to 1453\nData columns (total 61 columns):\ncashtag                   1454 non-null object\ncreated_at                1454 non-null int64\nofficial_account          1454 non-null float64\nsentiment                 1454 non-null int64\nsentiment score           1454 non-null float64\nsource                    1454 non-null object\nspans                     1454 non-null object\ntext                      1454 non-null object\ntotal_likes               1454 non-null float64\nclean_text                1454 non-null object\nbase_text                 1454 non-null object\nPOS_VB                    1454 non-null int64\nPOS_VBD                   1454 non-null int64\nPOS_VBG                   1454 non-null int64\nPOS_VBN                   1454 non-null int64\nPOS_VBP                   1454 non-null int64\nPOS_VBZ                   1454 non-null int64\n+num                      1454 non-null int64\n-num                      1454 non-null int64\n+num%                     1454 non-null int64\n-num%                     1454 non-null int64\nword_num                  1454 non-null int64\nordinal_num               1454 non-null int64\nnum-num                   1454 non-null int64\nnum-num%                  1454 non-null int64\nnum-num-num               1454 non-null int64\nnum/num                   1454 non-null int64\nonly_number               1454 non-null int64\ncall_+num%                1454 non-null int64\nput_+num%                 1454 non-null int64\nput_-num%                 1454 non-null int64\nbull                      1454 non-null int64\nbear                      1454 non-null int64\nnumber_of_!               1454 non-null int64\nnumber_of_?               1454 non-null int64\nnumber_of_$               1454 non-null int64\ncontinous_!               1454 non-null int64\ncontinous_?               1454 non-null int64\nAFINN_sum_score           1454 non-null float64\nAFINN_max_score           1454 non-null float64\nAFINN_min_score           1454 non-null float64\nAFINN_pos_ratio           1454 non-null float64\nAFINN_neg_ratio           1454 non-null float64\nBingLiu_pos_ratio         1454 non-null float64\nBingLiu_neg_ratio         1454 non-null float64\nNRC_Hashtag_sum_score     1454 non-null float64\nNRC_Hashtag_max_score     1454 non-null float64\nNRC_Hashtag_min_score     1454 non-null float64\nNRC_Hashtag_pos_ratio     1454 non-null float64\nNRC_Hashtag_neg_ratio     1454 non-null float64\nSentiWordNet_sum_score    1454 non-null float64\nSentiWordNet_max_score    1454 non-null float64\nSentiWordNet_min_score    1454 non-null float64\nSentiWordNet_pos_ratio    1454 non-null float64\nSentiWordNet_neg_ratio    1454 non-null float64\nAvg_TFIDF_1-grams         1454 non-null float64\nAvg_TFIDF_2-grams         1454 non-null float64\nAvg_TFIDF_3-grams         1454 non-null float64\nAvg_TFIDF_4-grams         1454 non-null float64\ncaps_word                 1454 non-null int64\nhashtags                  1454 non-null int64\ndtypes: float64(24), int64(31), object(6)\nmemory usage: 693.0+ KB\n"}],"source":["df.info()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wyCXsW1xmAsM"},"source":["# Data Split"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(df.drop(['sentiment score'],1), df['sentiment score'],stratify=df['source'],test_size=0.2)"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["X_train['sentiment score']=y_train\n","X_test['sentiment score']=y_test\n","X_train=X_train.reset_index()\n","X_test=X_test.reset_index()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bS1vQS4lcyel"},"source":["# Export"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"_umoDCo3c2Sm"},"outputs":[],"source":["X_train.to_csv('C:/Users/Louis Owen/Desktop/NLP_Stacking_Ensemble/df_train_final.csv')\n","X_test.to_csv('C:/Users/Louis Owen/Desktop/NLP_Stacking_Ensemble/df_test_final.csv')"]}]}